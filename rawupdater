#!/usr/bin/env python3
"""

Update files of raw.communitydragon.org

The update is configured using a JSON file with the following entries.
All paths are relative to the configuration file's directory.

    base_path -- base path for various files (default: `.`)
    export_path -- output directory for exported files (default: `export`)
    storage_paths -- storage paths to use, indexed by patch version(s)
      The following keys are supported:
        pbe -- storage for PBE export
        X.Y -- storage for patch X.Y
        X.Y-A.B -- storage for patches X.Y to A.B inclusive
        X.Y- -- storage for patches X.Y and after
      If `pbe` key is not defined, PBE support is disabled.
      Exactly one `X.Y-` must be defined.
      Values are paths suitable for `Storage.from_path()`.
    guess_hashes -- if true, guess hashes on new patch

The following files are used (expecting default path configuration):

    export/<version>/        exported files for a patch version
    export/pbe/              exported PBE files
    export/latest            symlink to last patch
    last-versions.live.txt   release versions of the last update
    last-versions.pbe.txt    release versions of the last PBE update
    update.lock              lock file to prevent concurrent updates

Two branches are defined for automatic updates: `pbe` and `live`.

"""
import os
import re
import json
import shutil
import logging
from typing import Optional
import git
import requests
import filelock
import cdragontoolbox
from cdragontoolbox.storage import (
    Storage,
    Patch,
    PatchVersion,
)
from cdragontoolbox.patcher import (
    PatcherPatchElement,
    PatcherRelease,
)
from cdragontoolbox.wad import (
    Wad,
)
from cdragontoolbox.export import (
    CdragonRawPatchExporter,
)
from cdragontoolbox.hashes import (
    hashfile_lcu,
    hashfile_game,
    LcuHashGuesser,
    GameHashGuesser,
)


logger = logging.getLogger("rawupdater")


class Updater:
    """
    Update raw storage files
    """

    def __init__(self, conf_path):
        with open(conf_path) as f:
            conf = json.load(f)

        conf_dir = os.path.dirname(conf_path)
        self.base = os.path.normpath(os.path.join(conf_dir, conf.get('base_path', '.')))
        self.export = os.path.join(self.base, conf.get('export_path', "export"))
        self.guess_hashes = bool(conf.get('guess_hashes', True))

        # parse storages configuration
        self.storage_path_pbe = None
        self.storage_path_live = None
        self.storage_paths = []  # [(v0, v1|None, path)]
        for key, path in conf['storage_paths'].items():
            path = os.path.normpath(os.path.join(conf_dir, path))
            if key == 'pbe':
                self.storage_path_pbe = path
            else:
                m = re.match(r'^(\d+\.\d+)-(\d+\.\d+)?$', key)
                if not m:
                    raise ValueError(f"invalid storages key: {key!r}")
                v0, v1 = m.groups()
                if v1:
                    self.storage_paths.append((PatchVersion(v0), PatchVersion(v1), path))
                elif self.storage_path_live is not None:
                    raise ValueError("duplicate storage path for live patch")
                else:
                    self.storage_paths.append((PatchVersion(v0), None, path))
                    self.storage_path_live = path
        self._storages = {}  # {"pbe"|"live"|PatchVersion: Storage}

    def lock(self):
        """Acquire lock to avoid parallel changes"""
        return filelock.FileLock(os.path.join(self.base, "update.lock"))

    def storage_for_version(self, version: PatchVersion) -> Optional[Storage]:
        """Return a storage for a given version, if available"""

        if version == 'main':
            version = 'pbe'

        if version in self._storages:
            return self._storages[version]

        if version == 'pbe':
            storage_path = self.storage_path_pbe
        else:
            for v0, v1, path in self.storage_paths:
                if v0 <= version and (v1 is None or version < v1):
                    storage_path = path
                    break
            else:
                storage_path = None

        if storage_path is None:
            storage = None
        else:
            storage = Storage.from_path(storage_path)
        self._storages[version] = storage
        return storage

    def storage_for_branch(self, branch) -> Storage:
        """Return a storage for a branch, fail if not available"""

        if branch in self._storages:
            return self._storages[branch]

        if branch == 'pbe':
            storage_path = self.storage_path_pbe
        elif branch == 'live':
            storage_path = self.storage_path_live

        if storage_path is None:
            raise ValueError(f"no storage configured for branch '{branch}'")

        storage = Storage.from_path(storage_path)
        self._storages[branch] = storage
        return storage

    def exported_versions(self):
        """Return a list of PatchVersion of exported patches

        Versions are sorted, PBE and versions without a configured storage are
        excluded.
        """

        versions = []
        for path in os.listdir(self.export):
            if path in ('latest', 'pbe'):
                continue
            if not os.path.isdir(os.path.join(self.export, path)):
                continue
            try:
                version = PatchVersion(path)
            except (ValueError, TypeError):
                continue
            if self.storage_for_version(version) is None:
                continue
            versions.append(version)

        return sorted(versions)

    def first_version(self):
        return min(v0 for v0, _, _ in self.storage_paths)

    def last_versions_path(self, branch):
        return os.path.join(self.base, f"last-versions.{branch}.txt")

    def get_last_versions(self, branch):
        """Return a `{channel: version}` map of versions used for the last update"""

        path = self.last_versions_path(branch)
        if not os.path.exists(path):
            return {}
        versions = {}
        with open(path) as f:
            for line in f:
                if not line:
                    continue
                channel, version = line.strip().split('=')
                versions[channel] = int(version)
        return versions

    def save_last_versions(self, branch, patch):
        """Update last versions from patch"""

        path = self.last_versions_path(branch)
        with open(path, 'w') as f:
            for channel, version in sorted(self.get_release_versions_for_patch(patch).items()):
                f.write(f"{channel}={version}\n")

    def get_release_versions_for_patch(self, patch):
        """Return `{channel: version}` map from a patch"""
        storage = self.storage_for_version(patch.version)
        if storage.storage_type == 'patcher':
            releases = [e.elem.release for e in patch.elements]
        elif storage.storage_type == 'multipatcher':
            releases = [e2.release for e in patch.elements for e2 in e.elements]
        else:
            raise TypeError(f"unsupported storage type: {storage.storage_type}")
        return {r.storage.channel: r.version for r in releases}

    def get_new_patch(self, branch):
        """Check if a new patch is available and return it"""

        last = self.get_last_versions(branch)
        storage = self.storage_for_branch(branch)
        patch = storage.patch(stored=False)
        # add the new patch version to storage if needed
        self._storages[patch.version] = storage

        new = self.get_release_versions_for_patch(patch)
        if any(v > last.get(ch, 0) for ch, v in new.items()):
            return patch
        return None

    def fetch_patch(self, patch):
        """Download a patch, guess new hashes if needed and return them"""

        logger.info(f"fetch new patch: {patch.version}")
        patch.download(langs=True)

        if not self.guess_hashes:
            return set()

        logger.info(f"guess new hashes from new patch: {patch.version}")

        for elem in patch.elements:
            if elem.name == 'client':
                lcu_wads = [Wad(p) for p in elem.fspaths() if p.endswith('.wad')]
                break
        else:
            raise ValueError("'client' patch element not found")

        for elem in patch.elements:
            if elem.name == 'game':
                game_wads = [Wad(p) for p in elem.fspaths() if p.endswith('.wad.client')]
                break
        else:
            raise ValueError("'game' patch element not found")

        # collect wads, and currently known hashes
        old_hashes = set(hashfile_lcu.load()) | set(hashfile_game.load())

        # guess LCU hashes
        guesser = LcuHashGuesser.from_wads(lcu_wads)
        if guesser.unknown:
            nunknown = len(guesser.unknown)
            for wad in guesser.wads:
                wad.guess_extensions()
                guesser.grep_wad(wad)
            guesser.substitute_numbers()
            guesser.substitute_extensions()

            nfound = nunknown - len(guesser.unknown)
            logger.info(f"new LCU hashes found: {nfound}")
            if nfound:
                guesser.save()

        # guess game hashes
        guesser = GameHashGuesser.from_wads(game_wads)
        if guesser.unknown:
            nunknown = len(guesser.unknown)
            for wad in guesser.wads:
                wad.guess_extensions()
                guesser.grep_wad(wad)
            guesser.substitute_numbers()
            guesser.substitute_extensions()
            guesser.check_basename_prefixes()

            nfound = nunknown - len(guesser.unknown)
            logger.info(f"new game hashes found: {nfound}")
            if nfound:
                guesser.save()

        return (set(hashfile_lcu.load()) | set(hashfile_game.load())) - old_hashes

    def export_new_patch(self, patch):
        """Export a (new) patch"""

        logger.info(f"export new patch: {patch.version}")
        is_pbe = patch.version == 'main'
        patch_exporter = self.patch_exporter_from_patch(patch)
        if is_pbe:
            # Create a previous patch with lastly exported files
            # This dramatically reduces export time for new patch
            # PatcherReleaseElement elements are assumed
            storage = self.storage_for_version('pbe')
            assert storage.storage_type == 'patcher'
            last_versions = self.get_last_versions('pbe')
            if last_versions:  # empty for first export
                assert len(last_versions) == 1 and storage.channel in last_versions
                prev_release = PatcherRelease(storage, last_versions[storage.channel])
                prev_patch_elements = [PatcherPatchElement(elem) for elem in prev_release.elements()]
                # note: symlinks is already forced to False
                patch_exporter.prev_patch = Patch._create(prev_patch_elements)
        patch_exporter.process(overwrite=True)

        # update the "latest" link if it exists (atomically)
        if not is_pbe:
            link_path = os.path.join(self.export, 'latest')
            target = str(patch.version)
            if os.path.islink(link_path) and os.readlink(link_path) != target:
                logger.info(f"update latest link to '{target}'")
                tmp_path = f"{link_path}.tmp"
                os.symlink(target, tmp_path)
                os.rename(tmp_path, link_path)

    def export_if_new_hashes(self, exporter, new_hashes):
        """Update an export if new hashes are available"""

        unknown_path = exporter.output + ".unknown.txt"
        if os.path.isfile(unknown_path):
            with open(unknown_path) as f:
                unknown = {int(h, 16) for h in f}
            if not unknown & new_hashes:
                return
        logger.debug(f"update exported patch for new hashes: {exporter.patch.version}")
        exporter.process(overwrite=False)

    def update_exports_for_new_hashes(self, new_hashes, pbe=True):
        """Update all exports for new hashes"""

        if not new_hashes:
            return

        for version in self.exported_versions():
            exporter = self.patch_exporter_from_version(version)
            self.export_if_new_hashes(exporter, new_hashes)

        if pbe:
            # allow to exclude PBE, to avoid updating for new hashes just before the full update
            exporter = self.patch_exporter_from_version('main')
            self.export_if_new_hashes(exporter, new_hashes)

    def update_exports(self, versions, overwrite=False):
        """Update exports, forcily"""

        if versions is None:
            # export everything
            versions = self.exported_versions()
            if self.storage_path_pbe is not None:
                versions.append('main')

        for version in versions:
            exporter = self.patch_exporter_from_version(version)
            exporter.process(overwrite=overwrite)

    def patch_exporter_from_patch(self, patch):
        """Return a PatchExporter for a given patch"""

        if patch.version == 'main':
            basename = 'pbe'
            previous_patch = None
            symlinks = False
        else:
            basename = str(patch.version)
            # retrieve previous patch version, if needed
            if patch.version == self.first_version():
                previous_patch = None
            else:
                storage = self.storage_for_version(patch.version)
                it = storage.patches(stored=True)
                for v in it:
                    if v.version == patch.version:
                        previous_patch = next(it)
                        break
                else:
                    raise ValueError(f"cannot guess previous patch for {patch}")
            symlinks = None
        return CdragonRawPatchExporter(os.path.join(self.export, basename), patch, previous_patch, symlinks)

    def patch_exporter_from_version(self, version):
        """Return a PatchExporter for a given version (or `main`)"""

        storage = self.storage_for_version(version)
        patch = storage.patch(version, stored=True)
        return self.patch_exporter_from_patch(patch)

    def clean_pbe_files(self, kept_patch):
        """Keep only the latest version of files"""

        storage = self.storage_for_version('main')

        logger.info("clean PBE files")

        # assume all elements are based on the same release
        release = kept_patch.elements[0].elem.release

        # remove extra manifests
        paths_to_keep = [os.path.basename(e.manif_url) for e in release.elements()]
        dir_path = storage.fspath("channels/public/releases")
        for p in os.listdir(dir_path):
            if p not in paths_to_keep:
                logger.debug(f"clean manifest {p}")
                os.remove(os.path.join(dir_path, p))

        # remove extra bundles
        paths_to_keep = {f"{i:016X}.bundle" for i in set.union(*(e.bundle_ids(langs=True) for e in release.elements()))}
        dir_path = storage.fspath("channels/public/bundles")
        for p in os.listdir(dir_path):
            if p not in paths_to_keep:
                logger.debug(f"clean bundle {p}")
                os.remove(os.path.join(dir_path, p))

        # remove extracted files
        release_dir = storage.fspath(release.storage_dir)
        dir_path = os.path.dirname(release_dir)
        for p in os.listdir(dir_path):
            if p != os.path.basename(release_dir):
                logger.debug(f"clean extracted files for version {p}")
                shutil.rmtree(os.path.join(dir_path, p))


def get_cdtb_repo():
    """Return a git.Repo for cdtb"""
    repo_dir = os.path.dirname(os.path.dirname(os.path.realpath(cdragontoolbox.__file__)))
    return git.Repo(repo_dir)

def git_commit_hashes(message):
    """Commit hashes changes"""

    repo = get_cdtb_repo()
    if repo.index.diff(repo.head.commit):
        logging.warning("cdtb repo index is not clean, won't commit hashes")
        return
    repo.index.add(['cdragontoolbox/hashes.lcu.txt'])
    repo.index.add(['cdragontoolbox/hashes.game.txt'])
    if not repo.index.diff(repo.head.commit):
        return  # no changes

    repo.index.commit(message)

def git_pull_hashes():
    """Update Git repository, check for hashes updates

    Return a set of new hashes.
    """

    logger.info("pull hashes update")
    repo = get_cdtb_repo()
    master = repo.active_branch
    if master.name != 'master':
        raise RuntimeError("local HEAD is not on 'master'")

    # fetch remote, check if there have ben new commits
    origin = repo.remote('origin')
    origin.fetch('master')
    tip_local = master.commit
    tip_remote = repo.rev_parse('origin/master')
    if tip_local == tip_remote:
        return set()  # no changes

    if not repo.is_ancestor(tip_local, tip_remote):
        raise RuntimeError("local HEAD (master) is not an ancestor of origin branch")

    # get new hashes, if any
    diff_index = tip_local.diff(tip_remote, paths=['cdragontoolbox/hashes.lcu.txt', 'cdragontoolbox/hashes.game.txt'])
    new_hashes = set()
    for diff in diff_index:
        hashes_old = {int(h.split()[0], 16) for h in diff.a_blob.data_stream.read().splitlines()}
        hashes_new = {int(h.split()[0], 16) for h in diff.b_blob.data_stream.read().splitlines()}
        new_hashes |= hashes_new - hashes_old

    # update local HEAD, use pull() to detect any error
    origin.pull('master', ff_only=True)

    return new_hashes


def command_new_patch(parser, args):
    updater = args.updater
    patch = updater.get_new_patch(args.branch)
    if not patch:
        return  # nothing to update

    new_hashes = updater.fetch_patch(patch)
    updater.update_exports_for_new_hashes(new_hashes, pbe=args.branch != 'pbe')
    updater.export_new_patch(patch)

    # commit changes to hashes.txt
    if new_hashes:
        if args.branch == 'pbe':
            git_commit_hashes("Update hashes (PBE update)")
        else:
            git_commit_hashes(f"Update hashes ({patch.version})")

    updater.save_last_versions(args.branch, patch)
    if args.branch == 'pbe':
        # Note: don't use storage.patch() to not clean the patch just extracted
        # if a new one has been made available since the beginning of the extract
        updater.clean_pbe_files(patch)


def command_update(parser, args):
    if args.patches:
        versions = ['main' if p == 'pbe' else p for p in args.patches]
        versions = [PatchVersion(v) for v in versions]
    else:
        versions = None

    updater = args.updater
    updater.update_exports(versions, overwrite=args.force)


def command_git_update(parser, args):
    new_hashes = git_pull_hashes()
    if not new_hashes:
        return

    args.updater.update_exports_for_new_hashes(new_hashes)


def main():
    import argparse

    parser = argparse.ArgumentParser()

    parser.add_argument('-v', '--verbose', action='count', default=0,
                        help="be verbose")
    parser.add_argument('-c', '--conf',
                        help="path to update configuration file")

    subparsers = parser.add_subparsers(dest='command', help="command")

    subparser = subparsers.add_parser('new-patch',
                                      help="check for new patch and export it")
    subparser.add_argument('branch', choices=['live', 'pbe'], default='live',
                           help="check for a new patch for the given branch")

    subparser = subparsers.add_parser('update',
                                      help="update given exports, for instance for new hashes")
    subparser.add_argument('-f', '--force', action='store_true', default=False,
                           help="Forcily overwrite files")
    subparser.add_argument('patches', nargs='*',
                           help="patches to export, none to update all, 'pbe' for PBE")

    args = parser.parse_args()

    if args.verbose >= 3:
        loglevel = logging.DEBUG
    elif args.verbose >= 2:
        loglevel = logging.INFO
    else:
        loglevel = logging.WARNING

    logging.basicConfig(
        level=loglevel,
        datefmt='%Y-%m-%d %H:%M:%S',
        format='%(asctime)s %(levelname)s %(name)s - %(message)s',
    )

    if args.verbose >= 2:
        logger.setLevel(logging.DEBUG)
        cdragontoolbox.logger.setLevel(logging.DEBUG)
    elif args.verbose >= 1:
        logger.setLevel(logging.INFO)
        cdragontoolbox.logger.setLevel(logging.INFO)

    args.updater = Updater(args.conf)
    with args.updater.lock():
        globals()[f"command_{args.command.replace('-', '_')}"](parser, args)


if __name__ == '__main__':
    main()
